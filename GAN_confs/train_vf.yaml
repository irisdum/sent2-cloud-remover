### YAML TO THE TRAIN PARAMETERS

train_directory: "/tmp/input_large_dataset/train/" #path to the dataset where the train data are stored, recommended to copy them on a temporary directory of the computer
val_directory: "/tmp/input_large_dataset/val/"  #path where the validation data are stored
## The rescaling between 0 and 1 procedure : recommended, the default set to null RGBN normalization band by band and VV,VH standardization
normalization: true #id set to true normalization to the data is apllied, the norm implemented is standardization for SAR data and normalization for RGBNIR
dict_band_x: {"VV":[0,2],"VH":[1,3],"R,G,B":[4,5,6],"NIR":[7]}#the stats for normalization procedure is going to be compute on each of these group of bands
dict_band_label: {"R,G,B":[0,1,2],"NIR":[3]}
dict_rescale_type: {"VV": "StandardScaler", "VH":"StandardScaler","R,G,B":"StandardScaler","NIR":"StandardScaler"}
s2_scale: 0.142
s1_scale: 0.2
s1bands: ["VV","VH"]
s2bands: ["R,G,B","NIR"]
lim_train_tile: 256 #set to an int to limit the values of the total training tiles taken, useful to train of fewer tiles to check if the code works well
lim_val_tile: null #set to an int to limit the values of the total validation tiles taken, useful to train of fewer tiles to check if the code works well

## Parameter for the Adam optimizer
lr: 0.0001 #learning rate
fact_g_lr: 1 #multiply lr in the Generator Adam optimiser
beta1: 0.5
lambda: 100 #100 is recoomended #lambda is the factor applied to the L1 loss abs(||G(z)-y||)


load_model: null #if not null is the iteration where we load our model, the training will start over at ite+1
epoch: 300 #epoch of train of the generator
batch_size: 2
steps_per_execution: 32 #inside train_on_batch_function set to 1 if you do want to use it
training_number: 0 #To modify, the checkpoints, logs will be saved in dir <path_training_dir>/training_<training_number>
training_dir: "/srv/osirim/idumeur/trainings/" #the model images and checkpoint will be stored at training_dir/trainining_number
# The model will be saved every saving step epoch
multi_gpu: true #the nber of GPU used for the training. Keep in mind that with multiple GPU, the weights are updated each
  # batch_size*n_device_avalaible tiles


im_saving_step: 1 #every n epochs image from the training set are saved
weights_saving_step: 1 # every n epochs the model is saved
metric_step: 1 #every n iterations when the validation metrics are computed. An iteration is the number of batches that goes into the Network


## DISCRIMINATOR PREVENT OVERFITTING
real_label_smoothing: [1.0,1.0] #if you do not want to apply label smoothing [1,1]
fake_label_smoothing: [0.0,0.0] #if you do not want to apply label smoothing [0,0]

sigma_init: 0.5 #used only if Gaussian noise used in the model definition
sigma_decay: 0.5 # multiply
sigma_step: 10 #every sigma_step iteration sigma decrease from sigma_decay

