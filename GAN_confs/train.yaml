### YAML TO THE TRAIN PARAMETERS

train_directory: "/datastore/dum031/data/dataset2/prepro1/input_dataset/train/"
val_directory: "/datastore/dum031/data/dataset2/prepro1/input_dataset/val/"
normalization: true
## Parameter for the Adam optimizer
lr: 0.0002
fact_g_lr: 1 #multiply lr in the Generator Adam optimiser
beta1: 0.5
lambda: 100 #lambda apply the constistency loss
## Path to the logdir tensorboard directory
logdir: "/datastore/dum031/models/logs/"


load_model: null #if not null is the iteration where we load our model, the training will start over at ite+1
epoch: 50 #epoch of train of the generator
k_step: 5 #means 5 epoch for training the discriminator before one training of generator
batch_size: 1
training_number: 1
training_dir: "/datastore/dum031/trainings/"
# The model will be saved every saving step epoch

im_saving_step: 30
weights_saving_step: 50
metric_step: 50 #steps when the validation metrics are computed
#TRAIN THE GENERATOR MULTIPLE TIME
train_g_multiple_time: [3,4,5,6,7,20,21,22,23]

## DISCRIMINATOR PREVENT OVERFITTING
real_label_smoothing: [0.7,1.0] #if you do not want to apply label smoothing [1,1]
fake_label_smoothing: [0.0,0.2] #if you do not want to apply label smoothing [0,0]

sigma_init: 0.5
sigma_decay: 0.5 # multiply
sigma_step: 10 #every sigma_step iteration sigma decrease from sigma_decay

##LOSS NOT USED WITH CLEAN_GAN
wasserstein: true
generator_loss: total_generatot_loss # corresponds to maximize log(D(G(x))+L1 loss  could be wasserstein_generator_loss
discriminator_loss: noisy_discriminator_loss  #It is more sigmoid_cross_entropy with label a random number between
#real_label_smoothing and discri_label_smoothing could be wasserstein_discriminator_loss