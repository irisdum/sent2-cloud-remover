### YAML TO THE TRAIN PARAMETERS

train_directory: "/datastore/dum031/data/dataset2/prepro1/input_dataset/train/"
## Parameter for the Adam optimizer
lr: 0.0002
fact_g_lr: 1 #multiply lr in the Generator Adam optimiser
beta1: 0.5
lambda: 100 #lambda apply the constistency loss
## Path to the logdir tensorboard directory
logdir: "/datastore/dum031/models/logs/"

epoch: 50 #epoch of train of the generator
k_step: 5 #means 5 epoch for training the discriminator before one training of generator
batch_size: 1
result_dir: "/datastore/dum031/trainings/training1/image_result/"
model_dir: "/datastore/dum031/trainings/training1/"
checkpoint_dir: null # if None no previous model is going to be downloaded
# The model will be saved every saving step epoch
saving_step: 30
n_train_image_saved: 1
label_smoothing: 0.9
